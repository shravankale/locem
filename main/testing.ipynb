{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, fnmatch, re, cv2, random,sys, pickle\n",
    "import torch\n",
    "#import imgaug as ia\n",
    "#import imageio\n",
    "%matplotlib inline  \n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "sys.path.append('..')\n",
    "\n",
    "root_locEm = \"../\"\n",
    "root_datasets = \"../../../../datasets/\"\n",
    "root_ImageNetVidsDevkit = root_datasets+\"ImageNetVids/imageNetVidsDevkit.data/\"\n",
    "root_ImageNetVids = root_datasets+\"ImageNetVids/imageNetVids.data/\"\n",
    "path_to_frames= root_ImageNetVids+\"Data/VID/train/\"\n",
    "path_to_val_frames= root_ImageNetVids+\"Data/VID/val/\"\n",
    "path_to_annotations= root_ImageNetVids+\"Annotations/VID/train/\"\n",
    "\n",
    "network_dim = 224\n",
    "map_vid = pd.read_pickle(\"../../data/map_vid.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genINV_Locem_Eval_v101 import ImageNetVID\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING v101 of generator\n",
      "USING v101 of generator\n"
     ]
    }
   ],
   "source": [
    "train_dataset = \"../../data/metadata_imgnet_vid_train_n2.pkl\"\n",
    "val_dataset = \"../../data/metadata_imgnet_vid_val_n2.pkl\"\n",
    "root_datasets = \"../../../../datasets/\"\n",
    "gen_train = ImageNetVID(root_datasets,train_dataset,split='train')\n",
    "gen_val = ImageNetVID(root_datasets,val_dataset,split='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "\n",
    "    '''print(\"TYPE DATA COLLATE\",type(data))\n",
    "    print(\"LEN DATA COLLATE\",len(data))\n",
    "    print(len(data[0]))\n",
    "    #print(data[0][0].size())\n",
    "    print(type(data[0][0]))\n",
    "    print(type(data[0][1]))\n",
    "    print(type(data[0][2]))\n",
    "    print(type(data[0][3]))\n",
    "    sys.exit(0)'''\n",
    "    \n",
    "    n = len(data[0])\n",
    "    print('n',n)\n",
    "    out = []\n",
    "\n",
    "    for i in range(n):\n",
    "        out.append(data[0][i])\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "        \n",
    "    images_list,target_list = [],[]\n",
    "    batch_size = len(data)\n",
    "    \n",
    "    for batch in range(batch_size):\n",
    "        images_list.append(data[batch][0])\n",
    "        target_list.append(data[batch][1])\n",
    "    \n",
    "    images = torch.cat(images_list,dim=0)\n",
    "    targets = torch.cat(target_list,dim=0)\n",
    "    \n",
    "    return images,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Length of the dataset to traverse 15806\n",
      "train Length of the dataset to traverse 15806\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(gen_train,batch_size=1,shuffle=True,collate_fn=collate_fn)\n",
    "val_loader = DataLoader(gen_val,batch_size=1,shuffle=False,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Length of the dataset to traverse 2618\n",
      "n 5\n",
      "image (480, 640, 3)\n",
      "bbox 1\n",
      "classname ['motorcycle']\n",
      "filename 000039\n",
      "TYPES\n",
      "image 480\n",
      "bbox 1\n",
      "classname 1\n",
      "filename 6\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i, (image,bbox,classname,filename,ids) in enumerate(val_loader):\n",
    "    print('image',image.shape)\n",
    "    print('bbox',len(bbox))\n",
    "    print('classname',classname)\n",
    "    print('filename',filename)\n",
    "\n",
    "    print(\"TYPES\")\n",
    "\n",
    "    print('image',len(image))\n",
    "    print('bbox',len(bbox))\n",
    "    print('classname',len(classname))\n",
    "    print('filename',len(filename))\n",
    "    \n",
    "    print(len(ids))\n",
    "    break\n",
    "    '''for b in range(len(bbox)):\n",
    "\n",
    "                x1,y1,x2,y2 = bbox[b]\n",
    "                targets_ev[(filename[b],classname[b])].append([x1,y1,x2,y2])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showTorchTensorImg(img):\n",
    "    \n",
    "    #assert img is a torch of size nchw\n",
    "    return plt.imshow(img.permute(1,2,0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs,tgt = [],[]\n",
    "for i,(images,target) in enumerate(train_loader):\n",
    "    imgs = images\n",
    "    tgt = target\n",
    "    print('Images size',images.size())\n",
    "    print('Target size',target.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S=7 \n",
    "B=2\n",
    "X=5\n",
    "C=30\n",
    "beta=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(2,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.normalize(a,p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = F.normalize(a,p=2)\n",
    "l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.sigmoid(a)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from r50_locem import resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o1 = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o1[0,0,0,40:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o1f = F.normalize(o1[:,:,:,40:],p=2)\n",
    "o1f[0,0,0,40:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o1f.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o1[0,0,0,40:].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o1f[0,0,0,40:].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randint(low=-5,high=5,size=(1,4),dtype=torch.float)\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = torch.randn((1,4))\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = F.normalize(p,p=2)\n",
    "p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = torch.zeros((1,4))\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cosine_similarity(p,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from r50_locem import resnet101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =resnet101(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = pd.read_pickle('../../data/metadata_imgnet_vid_val_n2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(dt)\n",
    "unique_ids = dataset.drop_duplicates(subset=['cat_code','snip_id','trackid'],keep='first')\n",
    "unique_ids = unique_ids.reset_index(drop=True)\n",
    "unique_ids = unique_ids.drop(labels=['folder','file','width','height','wnid','xmax','xmin','ymax','ymin'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = unique_ids[(unique_ids.cat_code==9) & (unique_ids.snip_id=='00102000') & (unique_ids.trackid==0)].index.to_numpy()\n",
    "ans = int(ans)\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = unique_ids.loc[0]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids[(unique_ids.cat_code==s.cat_code) & (unique_ids.snip_id==s.snip_id) & (unique_ids.cat_code==s.trackid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
