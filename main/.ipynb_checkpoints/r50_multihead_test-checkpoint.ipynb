{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, fnmatch, re, cv2, random,sys, pickle\n",
    "import torch\n",
    "#import imgaug as ia\n",
    "#import imageio\n",
    "%matplotlib inline  \n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "sys.path.append('..')\n",
    "\n",
    "root_locEm = \"../\"\n",
    "root_datasets = \"../../../../datasets/\"\n",
    "root_ImageNetVidsDevkit = root_datasets+\"ImageNetVids/imageNetVidsDevkit.data/\"\n",
    "root_ImageNetVids = root_datasets+\"ImageNetVids/imageNetVids.data/\"\n",
    "path_to_frames= root_ImageNetVids+\"Data/VID/train/\"\n",
    "path_to_val_frames= root_ImageNetVids+\"Data/VID/val/\"\n",
    "path_to_annotations= root_ImageNetVids+\"Annotations/VID/train/\"\n",
    "\n",
    "network_dim = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genINV_Locem_v101 import ImageNetVID\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataset = \"../../data/metadata_imgnet_vid_train_n2.pkl\"\n",
    "\n",
    "def collate_fn(data):\n",
    "        \n",
    "    images_list,target_list = [],[]\n",
    "    batch_size = len(data)\n",
    "    \n",
    "    for batch in range(batch_size):\n",
    "        images_list.append(data[batch][0])\n",
    "        target_list.append(data[batch][1])\n",
    "    \n",
    "    images = torch.cat(images_list,dim=0)\n",
    "    targets = torch.cat(target_list,dim=0)\n",
    "    \n",
    "    return images,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING v101 of generator\n",
      "Invoking augmentor\n"
     ]
    }
   ],
   "source": [
    "gen_train = ImageNetVID(root_datasets,train_dataset,split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Length of the dataset to traverse 15806\n",
      "train Length of the dataset to traverse 15806\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(gen_train,batch_size=1,shuffle=True,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Length of the dataset to traverse 15806\n",
      "sample_bbox size torch.Size([1, 4])\n",
      "sample_class size torch.Size([1, 1])\n",
      "images.size torch.Size([3, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for i, (images,targets) in enumerate(train_loader):\n",
    "    print('images.size',images.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOtherObjects(sample,train_data,all_data):\n",
    "    \n",
    "    boxes = []\n",
    "    classes = []\n",
    "    \n",
    "    other_samples = all_data[(all_data.folder==sample.folder)&(all_data.file==sample.file)]\n",
    "    #If there is only 1 object in image then there is nothing to add\n",
    "    if len(other_samples)==1:\n",
    "        return [],[]\n",
    "    #Dropping sample from other\n",
    "    other_samples = other_samples[other_samples.trackid!=sample.trackid]\n",
    "    \n",
    "    for idx in other_samples.index:\n",
    "        row = row.loc[idx]\n",
    "        box.append([row.xmin,row.ymin,row.xmax,row.ymax])\n",
    "        classes.append([row.cat_code-1])\n",
    "    \n",
    "    boxes = torch.tensor(boxes)\n",
    "    classes = torch.tensor(classes)\n",
    "    \n",
    "    return boxes,classes\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=[[2],[3],[4]]\n",
    "bt = torch.tensor(b)\n",
    "bt.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = pd.read_pickle('../../data/metadata_imgnet_vid_train_n2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad = pd.read_pickle('../../data/metadata_imgnet_vid_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getOtherObjects(td.loc[idx],td,ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_file = td.loc[idx].file\n",
    "folder = td.loc[idx].folder\n",
    "folder,img_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nad = ad[(ad.folder==folder)&(ad.file==img_file)]\n",
    "nad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.imread(path_to_frames+folder+'/'+img_file+'.JPEG'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "n = 0\n",
    "for i in range(10):\n",
    "    sample = td.loc[i]\n",
    "    nad = ad[(ad.folder==sample.folder)&(ad.file==sample.file)]\n",
    "    if len(nad) >1:\n",
    "        s=sample\n",
    "        n=nad\n",
    "        print(nad)\n",
    "        print('\\n')\n",
    "        print('sample')\n",
    "        print(sample)\n",
    "        plt.imshow(cv2.imread(path_to_frames+sample.folder+'/'+sample.file+'.JPEG'))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nm = n[n.trackid!=s.trackid]\n",
    "nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in n.index:\n",
    "    row = n.loc[i]\n",
    "    print(row.xmin,row.ymin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
